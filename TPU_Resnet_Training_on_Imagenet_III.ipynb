{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU Resnet Training on Imagenet III",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/christianmerkwirth/colabs/blob/master/TPU_Resnet_Training_on_Imagenet_III.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ty5g5lT2zpVz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d8a1025a-9d84-4d9d-e6e5-1071f1c8e93c"
      },
      "cell_type": "code",
      "source": [
        " # colab.research.google.com specific\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  import json\n",
        "  import os\n",
        "  from google.colab import auth\n",
        "\n",
        "  !git clone https://github.com/tensorflow/tpu.git\n",
        "  !mv tpu/models/official ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tpu'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 2218 (delta 55), reused 92 (delta 39), pack-reused 2061\u001b[K\n",
            "Receiving objects: 100% (2218/2218), 1.40 MiB | 12.77 MiB/s, done.\n",
            "Resolving deltas: 100% (1307/1307), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ih0PCgV-0sI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now run the main to set up flags and constant. The main function should not be executed.\n",
        "\n",
        "from official.resnet import resnet_main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BFftrYfhfZIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d419e1c0-0b24-4b78-b70a-c798e7939fb9"
      },
      "cell_type": "code",
      "source": [
        "# This cell contains the Colab-specific setup and hacks.\n",
        "\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from absl import flags\n",
        "import absl.logging as _logging\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth\n",
        "\n",
        "  # Authenticate to access GCS bucket\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  # Initiate fake FLAGS parsing.\n",
        "  FLAGS(['resnet_trainer'])\n",
        "  \n",
        "  # When connected to the TPU runtime\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    tpu_grpc = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "    FLAGS.tpu = tpu_grpc\n",
        "    FLAGS.use_tpu = True\n",
        "\n",
        "    # Upload credentials to the TPU\n",
        "    with tf.Session(tpu_grpc) as sess:\n",
        "      data = json.load(open('/content/adc.json'))\n",
        "      tf.contrib.cloud.configure_gcs(sess, credentials=data)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1023 09:11:59.163012 140486750078848 _default.py:280] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "57bqoXEAcPiv",
        "colab_type": "code",
        "outputId": "16f0e2a2-d53d-46a9-ca2c-d68af46ca01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "from official.resnet import imagenet_input\n",
        "from official.resnet import lars_util\n",
        "from official.resnet import resnet_model\n",
        "\n",
        "\n",
        "from tensorflow.python.estimator import estimator\n",
        "      \n",
        "# Number of training and evaluation images in the standard ImageNet dataset\n",
        "NUM_TRAIN_IMAGES = 1281167\n",
        "NUM_EVAL_IMAGES = 50000    \n",
        "\n",
        "# Change parameters according to your setup.\n",
        "FLAGS.model_dir = 'gs://tpu-cmerk-2/imagenet/models/resnet/20181022_007'\n",
        "FLAGS.data_dir = 'gs://tpu-cmerk-2/imagenet/train'\n",
        "\n",
        "LR_SCHEDULE = [    # (multiplier, epoch to start) tuples for piecewise linear lr\n",
        "      (0.001, 0),  (1.0, 4), (0.1, 21), (0.01, 35), (0.001, 43), (0.001, 1000)\n",
        "]\n",
        "def piecewise_linear_lr_schedule(current_epoch):\n",
        "  \"\"\"\n",
        "  Piecewise linear learning rate.\n",
        "  Args:\n",
        "    current_epoch: `Tensor` for current epoch.\n",
        "  Returns:\n",
        "    A scaled `Tensor` for current learning rate.\n",
        "  \"\"\"\n",
        "  scaled_lr = 1.00\n",
        "  lr = 0.0\n",
        "  for i in range(1, len(LR_SCHEDULE)):\n",
        "    this_lr = scaled_lr * ((LR_SCHEDULE[i][0] - LR_SCHEDULE[i-1][0]) /\n",
        "                           (LR_SCHEDULE[i][1] - LR_SCHEDULE[i-1][1]) *\n",
        "                           (current_epoch - LR_SCHEDULE[i-1][1]) +\n",
        "                           LR_SCHEDULE[i-1][0])\n",
        "    lr = tf.where(current_epoch >= LR_SCHEDULE[i-1][1], this_lr, lr)\n",
        "  return lr\n",
        "\n",
        "resnet_main.learning_rate_schedule = piecewise_linear_lr_schedule\n",
        "\n",
        "\n",
        "# Convert flags to dict of params so the model_fn can use TPU specific settings.\n",
        "params = FLAGS.flag_values_dict()\n",
        "    \n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu)\n",
        "\n",
        "config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=FLAGS.model_dir,\n",
        "    save_checkpoints_steps=FLAGS.iterations_per_loop,\n",
        "    keep_checkpoint_max=None,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "        num_shards=FLAGS.num_cores,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "# Input pipelines are slightly different (with regards to shuffling and\n",
        "# preprocessing) between training and evaluation.\n",
        "imagenet_train = imagenet_input.ImageNetInput(\n",
        "    is_training=True,\n",
        "    data_dir=FLAGS.data_dir,\n",
        "    use_bfloat16=True,\n",
        "    transpose_input=FLAGS.transpose_input)\n",
        "imagenet_eval = imagenet_input.ImageNetInput(\n",
        "    is_training=False,\n",
        "    data_dir=FLAGS.data_dir,\n",
        "    use_bfloat16=True,\n",
        "    transpose_input=FLAGS.transpose_input)\n",
        "\n",
        "resnet_classifier = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    model_fn=resnet_main.resnet_model_fn,\n",
        "    config=config,\n",
        "    train_batch_size=FLAGS.train_batch_size,\n",
        "    eval_batch_size=FLAGS.eval_batch_size,\n",
        "    params=params)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1023 09:12:05.666531 140486750078848 tf_logging.py:115] Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.30.223.98:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': None, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc5674afad0>, '_model_dir': 'gs://tpu-cmerk-2/imagenet/models/resnet/20181022_007', '_protocol': None, '_save_checkpoints_steps': 1251, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tpu_config': TPUConfig(iterations_per_loop=1251, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7fc5674af910>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_evaluation_master': 'grpc://10.30.223.98:8470', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': 'grpc://10.30.223.98:8470'}\n",
            "I1023 09:12:05.669792 140486750078848 tf_logging.py:115] _TPUContext: eval_on_tpu True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HSuSD_H3XWrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "current_step = estimator._load_global_step_from_checkpoint_dir(FLAGS.model_dir)\n",
        "batches_per_epoch = NUM_TRAIN_IMAGES / FLAGS.train_batch_size\n",
        "print('Training for %d steps (%.2f epochs in total). Current step %d.' % \n",
        "      (FLAGS.train_steps, FLAGS.train_steps / batches_per_epoch, current_step))\n",
        "\n",
        "resnet_classifier.train(input_fn=imagenet_train.input_fn, max_steps=FLAGS.train_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H-GWwCat4Ve6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "896c2023-5482-4bdc-e448-4f921982ab1a"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "eval_steps = NUM_EVAL_IMAGES // FLAGS.eval_batch_size\n",
        "\n",
        "print('Starting to evaluate.')\n",
        "eval_start = time.time()  # This time will include compilation time\n",
        "eval_results = resnet_classifier.evaluate(\n",
        "    input_fn=imagenet_eval.input_fn,\n",
        "    steps=eval_steps)\n",
        "eval_time = int(time.time() - eval_start)\n",
        "print('Eval results: %s. Elapsed seconds: %d' % (eval_results, eval_time))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I1023 09:12:20.592689 140486750078848 tf_logging.py:115] Querying Tensorflow master (grpc://10.30.223.98:8470) for TPU system metadata.\n",
            "I1023 09:12:20.605627 140486750078848 tf_logging.py:115] Found TPU system:\n",
            "I1023 09:12:20.607089 140486750078848 tf_logging.py:115] *** Num TPU Cores: 8\n",
            "I1023 09:12:20.612056 140486750078848 tf_logging.py:115] *** Num TPU Workers: 1\n",
            "I1023 09:12:20.614027 140486750078848 tf_logging.py:115] *** Num TPU Cores Per Worker: 8\n",
            "I1023 09:12:20.615044 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4081254722591740278)\n",
            "I1023 09:12:20.618617 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3870344600991421031)\n",
            "I1023 09:12:20.619668 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 16846320029899946671)\n",
            "I1023 09:12:20.622309 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14912435175050231260)\n",
            "I1023 09:12:20.623456 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13759951883248168723)\n",
            "I1023 09:12:20.624644 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1220539578848903611)\n",
            "I1023 09:12:20.627024 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 8701637448952871799)\n",
            "I1023 09:12:20.627706 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7544941491723015294)\n",
            "I1023 09:12:20.628820 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4945713875414777715)\n",
            "I1023 09:12:20.630665 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15601591775711563693)\n",
            "I1023 09:12:20.631252 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 11372755882129004910)\n",
            "I1023 09:12:20.632392 140486750078848 tf_logging.py:115] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 7600573707535534228)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting to evaluate.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I1023 09:12:22.125166 140486750078848 tf_logging.py:115] Calling model_fn.\n",
            "W1023 09:12:22.257639 140486750078848 tf_logging.py:125] From official/resnet/imagenet_input.py:293: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W1023 09:12:22.302369 140486750078848 tf_logging.py:125] From official/resnet/imagenet_input.py:180: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "I1023 09:12:25.992311 140486750078848 tf_logging.py:115] Done calling model_fn.\n",
            "I1023 09:12:26.016001 140486750078848 tf_logging.py:115] Starting evaluation at 2018-10-23-09:12:26\n",
            "I1023 09:12:26.017507 140486750078848 tf_logging.py:115] TPU job name worker\n",
            "I1023 09:12:26.663330 140486750078848 tf_logging.py:115] Graph was finalized.\n",
            "I1023 09:12:26.788727 140486750078848 tf_logging.py:115] Restoring parameters from gs://tpu-cmerk-2/imagenet/models/resnet/20181022_007/model.ckpt-112590\n",
            "I1023 09:12:28.941963 140486750078848 tf_logging.py:115] Running local_init_op.\n",
            "I1023 09:12:29.033010 140486750078848 tf_logging.py:115] Done running local_init_op.\n",
            "I1023 09:12:29.257370 140486750078848 tf_logging.py:115] Init TPU system\n",
            "I1023 09:12:36.733181 140486750078848 tf_logging.py:115] Initialized TPU in 7 seconds\n",
            "I1023 09:12:36.735383 140485726549760 tf_logging.py:115] Starting infeed thread controller.\n",
            "I1023 09:12:36.745753 140485718157056 tf_logging.py:115] Starting outfeed thread controller.\n",
            "I1023 09:12:37.120935 140486750078848 tf_logging.py:115] Initialized dataset iterators in 0 seconds\n",
            "I1023 09:12:37.215387 140486750078848 tf_logging.py:115] Enqueue next (48) batch(es) of data to infeed.\n",
            "I1023 09:12:37.217328 140486750078848 tf_logging.py:115] Dequeue next (48) batch(es) of data from outfeed.\n",
            "I1023 09:12:53.506340 140486750078848 tf_logging.py:115] Evaluation [48/48]\n",
            "I1023 09:12:53.507731 140486750078848 tf_logging.py:115] Stop infeed thread controller\n",
            "I1023 09:12:53.511071 140486750078848 tf_logging.py:115] Shutting down InfeedController thread.\n",
            "I1023 09:12:53.513462 140485726549760 tf_logging.py:115] InfeedController received shutdown signal, stopping.\n",
            "I1023 09:12:53.516165 140485726549760 tf_logging.py:115] Infeed thread finished, shutting down.\n",
            "I1023 09:12:53.517710 140486750078848 tf_logging.py:115] infeed marked as finished\n",
            "I1023 09:12:53.519870 140486750078848 tf_logging.py:115] Stop output thread controller\n",
            "I1023 09:12:53.523550 140486750078848 tf_logging.py:115] Shutting down OutfeedController thread.\n",
            "I1023 09:12:53.529165 140485718157056 tf_logging.py:115] OutfeedController received shutdown signal, stopping.\n",
            "I1023 09:12:53.535046 140485718157056 tf_logging.py:115] Outfeed thread finished, shutting down.\n",
            "I1023 09:12:53.536595 140486750078848 tf_logging.py:115] outfeed marked as finished\n",
            "I1023 09:12:53.538007 140486750078848 tf_logging.py:115] Shutdown TPU system.\n",
            "I1023 09:12:53.954890 140486750078848 tf_logging.py:115] Finished evaluation at 2018-10-23-09:12:53\n",
            "I1023 09:12:53.956223 140486750078848 tf_logging.py:115] Saving dict for global step 112590: global_step = 112590, loss = 1.3765411, top_1_accuracy = 0.7579956, top_5_accuracy = 0.9286499\n",
            "I1023 09:12:57.380568 140486750078848 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 112590: gs://tpu-cmerk-2/imagenet/models/resnet/20181022_007/model.ckpt-112590\n",
            "I1023 09:12:58.002499 140486750078848 tf_logging.py:115] evaluation_loop marked as finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval results: {'loss': 1.3765411, 'top_1_accuracy': 0.7579956, 'global_step': 112590, 'top_5_accuracy': 0.9286499}. Elapsed seconds: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uxrdDHoscF3x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}